<p align="center">
  <img src="./img.png" alt="Project Banner" width="100%">
</p>

# [AI_Grooming_Detection] üéØ

## Basic Details

### Team Name: [Nexa]

### Team Members
- Member 1: [A.J.Athulya] - [College of Engineering and Management Punnapra]
- Member 2: [Sherin Eabdhi H] - [College Of Engineering and Managemnet Punnapra]

### Hosted Project Link
[https://ajathulya.pythonanywhere.com/]

### Project Description
[AI Guardian is an intelligent digital safety system designed to detect grooming, emotional manipulation, harassment, threats, and self-harm risks in online conversations.
It analyzes messages using behavioral pattern detection and risk scoring to identify harmful intent and escalation.
The system provides real-time protective guidance, suggested safe responses, and risk alerts to enhance women‚Äôs digital safety.]

### The Problem statement
[Online communication platforms have increasingly become spaces where women are exposed to grooming, emotional manipulation, harassment, threats, and psychological exploitation. Existing moderation systems primarily rely on basic keyword filtering and often fail to detect subtle behavioral patterns, escalation, and context-based risks. There is a need for an intelligent, real-time digital safety system that can analyze conversations, identify harmful intent, and provide protective guidance to enhance women‚Äôs online safety.]

### The Solution
[AI Guardian proposes a hybrid intelligent safety system that analyzes online conversations to detect grooming, emotional manipulation, harassment, threats, and self-harm risks using behavioral pattern recognition and rule-based risk scoring. The system classifies the severity level into Guardian Modes (Calm, Alert, Defense, Emergency) and provides real-time protective guidance and suggested safe responses. By combining contextual detection with escalation analysis, AI Guardian acts as a digital safety bodyguard to enhance women‚Äôs online security and awareness.]

---

## Technical Details

### Technologies
- Languages used: [Python,HTML,CSS]
- Frameworks used: [flask]
- Libraries used: [Flask, re (Regular Expressions), nltk (optional for NLP enhancement)]
- Tools used: [VS Code, Git, GitHub, Replit/Render (for hosting)]

---

## Features

List the key features of your project:
- Feature 1: [Behavioral Risk Detection
Detects grooming, threats, harassment, emotional manipulation, secrecy patterns, and self-harm risks in conversations]
- Feature 2: [Guardian Mode Classification
Classifies risk into four safety levels: Calm, Alert, Defense, and Emergency]
- Feature 3: [Protective Response Suggestion
Suggests safe and assertive replies to protect users from manipulation or harmful interaction.]
- Feature 4: [Hybrid Intelligent Rule Engine
Uses phrase-based detection, escalation scoring, and contextual risk logic for more accurate safety analysis.]

---

## Implementation

#### Installation
git clone https://github.com/Athulya001/AI_GROOMING_DETECTION
cd AI_Grooming_Detection
pip install -r requirements.txt

#### Run

python app.py

## Project Documentation

#### Screenshots (Add at least 3)

https://drive.google.com/file/d/1U_WNmD8oTQmXCv8aj8yUZOoGW0AhGB43/view?usp=drive_link
Homepage interface showing AI Guardian activation input field

https://drive.google.com/file/d/1NzDT1RZWTriF5gOdLiqoKxQygvDxw3UQ/view?usp=drive_link
Safe mode

https://drive.google.com/file/d/1QHXqwx6sS1AyPMpkgrNlNKvFTaX7cvDE/view?usp=drive_link
Emergency mode triggered when self-harm or severe threats are detected

#### Diagrams

**System Architecture:**

![Architecture Diagram](https://drive.google.com/file/d/1WxlKHmF2in52nqLmTP8RYY2iK8go2HaI/view?usp=drive_link)
AI Guardian uses a three-layer architecture consisting of a frontend interface, a Flask backend server, and a hybrid risk detection engine.
User input is processed by the backend where behavioral patterns and risk levels are analyzed.
The system then returns Guardian Mode classification and protective suggestions to the user interface.

**Application Workflow:**

![Workflow](https://drive.google.com/file/d/1jVbMZz3qyh8lrXlM3tzuVVtZBzllyUKW/view?usp=drive_link)
The user submits conversation text through the web interface for analysis.
The system preprocesses the input, detects harmful patterns such as grooming, threats, or manipulation, and calculates a risk score.
Based on severity, it classifies the interaction into Calm, Alert, Defense, or Emergency mode and provides safety guidance.
---

### For Scripts/CLI Tools:

#### Command Reference

**Basic Usage:**

python app.py 


**Available Commands:**
- `command1 [python app.py]` - Starts the AI Guardian server

**Options:**
- `-h, --help` - Show help message and exit
- `-v, --verbose` - Enable verbose output
- `-o, --output FILE` - Specify output file path
- `-c, --config FILE` - Specify configuration file
- `--version` - Show version information

**Examples:**


# Example: Basic usage
python script.py input.txt


#### Demo Output

**Example: Basic Processing**

**Input:**

Send me your private picture

**Command:**




**Output:**
Guardian Mode: DEFENSE
Detected Risk: Grooming / Boundary Violation
Suggested Response:
"I am not comfortable with this. Please stop immediately."

## Project Demo

### Video
https://drive.google.com/drive/folders/1y8F9uXZzH_mF4RKFbIvsbOoGZuUAJ0Re?usp=drive_link
Real-time risk detection

Grooming identification

Harassment detection

Emergency self-harm detection

Guardian Mode switching

### Additional Demos
[Add any extra demo materials/links - Live site, APK download, online demo, etc.]

---

## Team Contributions

- [A.J.Athulya]: [ Backend development, Risk detection logic, System design]
- [Sherin Ebadhi H]: [Frontend UI design, Styling, User experience]
---

## License

This project is licensed under the [MIT] License - see the [LICENSE](LICENSE) file for details.

---

Made with ‚ù§Ô∏è at TinkerHub
